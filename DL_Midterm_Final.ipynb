{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Set up / Load Model and Data"
      ],
      "metadata": {
        "id": "7TK5lPZJ9DxZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EwFyExva8-Rq"
      },
      "outputs": [],
      "source": [
        "# Set up Code\n",
        "# This cell will take time\n",
        "!pip install unsloth\n",
        "# Also get the latest nightly Unsloth!\n",
        "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 2048 # Choose any\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False."
      ],
      "metadata": {
        "id": "S-ic2p4j9DR0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load original model (Run this cell to start from scratch)\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Meta-Llama-3.1-8B\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")\n",
        "\n",
        "from peft import LoftQConfig, LoraConfig\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 64, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128 # 16 default\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 64,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = LoftQConfig(loftq_bits=4), # And LoftQ\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "rlyG0s_b9R-t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load pre-trained model (Run this cell to load already trained version)\n",
        "\n",
        "# load fine-tuned model from huggingface\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"suzoliver/Nov14_r64\", # Name of HuggingFace directory\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference"
      ],
      "metadata": {
        "id": "1hIigJF29XM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# download and load competition dataset\n",
        "\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"ad6398/nyu-dl-teach-maths-comp\")\n",
        "# print and see dataset\n",
        "dataset"
      ],
      "metadata": {
        "id": "HKRlxkzn9lwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Partition dataset and prep for training"
      ],
      "metadata": {
        "id": "Sjam61RY9m5n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set prompt for LLM\n",
        "\n",
        "prompt = \"\"\"Tell me if the answer to the following math question is correct or not. You can only respond 'True' or 'False'. Below is Question and Answer and an Explanation of the Answer.\n",
        "### Question:\n",
        "{}\n",
        "\n",
        "### Answer:\n",
        "{}\n",
        "\n",
        "## Explanation:\n",
        "{}\n",
        "\n",
        "### Output:\n",
        "{}\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
        "def formatting_prompts_func(examples):\n",
        "    question = examples[\"question\"]\n",
        "    ans       = examples[\"answer\"]\n",
        "    explan    = examples[\"solution\"]\n",
        "    output      = examples[\"is_correct\"]\n",
        "    texts = []\n",
        "    for instruction, input, explan, output in zip(question, ans, explan, output):\n",
        "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
        "        text = prompt.format(instruction, input, explan, output) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return { \"text\" : texts, }"
      ],
      "metadata": {
        "id": "iq4INDoh9sPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parition dataset into train and validation\n",
        "\n",
        "# adjust train_size depending on how many points are needed for training\n",
        "split_data = dataset['train'].train_test_split(train_size = 30000, shuffle=False)\n",
        "\n",
        "# get val_data from test split of split_data, adjust train_size for the size of the validation set\n",
        "val_data = split_data['test'].train_test_split(train_size = 1000, shuffle=True)"
      ],
      "metadata": {
        "id": "ByeHUFrJ97TN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process the training dataset and generate prompt for each datapoint\n",
        "\n",
        "train_dataset = split_data['train'].map(formatting_prompts_func, batched = True)\n",
        "val_dataset = val_data['train'].map(formatting_prompts_func, batched = True,)"
      ],
      "metadata": {
        "id": "4NYmthZC-VkH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Garbage Collector"
      ],
      "metadata": {
        "id": "T5LK37jN-ZLP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# garbage collector to run when facing out-of-memory issues\n",
        "\n",
        "import gc\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "hTeSEO_r-bQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "eBT9Tr_M-iCO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "# Set training arguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        per_device_eval_batch_size = 8, # highest possible without tripping colab memory error\n",
        "        gradient_accumulation_steps = 16,\n",
        "        warmup_steps = 5,\n",
        "        num_train_epochs = 1, # Set this for 1 full training run.\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\", # Use this for WandB etc\n",
        "        eval_steps = 20, # only do validation every 20 steps because it is slows down training\n",
        "        eval_strategy=\"steps\"\n",
        "    )\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = train_dataset,\n",
        "    eval_dataset = val_dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 4,\n",
        "    packing = False, # Can make training 5x faster for short sequences.\n",
        "    args = training_args,\n",
        ")"
      ],
      "metadata": {
        "id": "6FuKqv6Q-mvC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run code and store training and validation losses\n",
        "\n",
        "trainer_stats = trainer.train()"
      ],
      "metadata": {
        "id": "Zyqd3G-R-1me"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save training and validation losses to csv\n",
        "import csv\n",
        "\n",
        "with open('trainHist.csv', mode='w', newline='') as file:\n",
        "  # Create a CSV writer object\n",
        "  csv_writer = csv.writer(file)\n",
        "\n",
        "  # Write any row with 'loss' (training loss) to csv\n",
        "  for i in range(len(trainer.state.log_history)):\n",
        "    if 'loss' in trainer.state.log_history[i].keys():\n",
        "      csv_writer.writerow(trainer.state.log_history[i].values())\n",
        "\n",
        "with open('valHist.csv', mode='w', newline='') as file:\n",
        "  # Create a CSV writer object\n",
        "  csv_writer = csv.writer(file)\n",
        "\n",
        "  # Write any row with 'eval_loss' (validation loss) to csv\n",
        "  for i in range(len(trainer.state.log_history)):\n",
        "    if 'eval_loss' in trainer.state.log_history[i].keys():\n",
        "      csv_writer.writerow(trainer.state.log_history[i].values())"
      ],
      "metadata": {
        "id": "XjHrfxsS_PfO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference"
      ],
      "metadata": {
        "id": "9q4gqpVo_kro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create prompt without output so that model can provide answer\n",
        "\n",
        "def formatting_prompts_func2(examples):\n",
        "    question = examples[\"question\"]\n",
        "    ans       = examples[\"answer\"]\n",
        "    explan    = examples[\"solution\"]\n",
        "    texts = []\n",
        "    for instruction, explan, input in zip(question, explan, ans):\n",
        "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
        "        text = prompt.format(instruction, input, explan, \"\")\n",
        "        texts.append(text)\n",
        "    return { \"text\" : texts, }\n",
        "\n",
        "# use this line for validation\n",
        "#val_temp = split_data['test'].train_test_split(test_size = 1000, shuffle=False)\n",
        "\n",
        "# use this line to predict on test set\n",
        "val_temp = dataset\n",
        "\n",
        "val_dataset = val_temp['test'].map(formatting_prompts_func2, batched = True)"
      ],
      "metadata": {
        "id": "auwuZXWg_mVr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create pipeline for inferencing\n",
        "\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "from transformers import pipeline\n",
        "pl = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=4)"
      ],
      "metadata": {
        "id": "DKkscYpwAFY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# do inference with pipeline and store in ans variable\n",
        "\n",
        "nData = 10000; # 10000 for testing, 1000 for validation set\n",
        "\n",
        "ans = [0] * nData\n",
        "for i, out in enumerate(pl(val_dataset[\"text\"][0:nData], return_full_text=False, batch_size=8)):\n",
        "  ans[i] = [i,out[0]['generated_text']]\n"
      ],
      "metadata": {
        "id": "v7bhtQYCAI9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert inference output to booleans\n",
        "\n",
        "ansBool = ans\n",
        "for i in range(nData):\n",
        "  # convert string to boolean for comparison with is_correct field\n",
        "  if ans[i][1] == 'True' or ans[i][1] == 'False':\n",
        "    ansBool[i][1] = eval(ans[i][1])\n",
        "  else:\n",
        "    # if the output is something other than 'True' or 'False', guess 'True'\n",
        "    # note, this should only happen on models that aren't sufficiently trained\n",
        "    ansBool[i][1] = True\n",
        "\n",
        "print(ansBool)"
      ],
      "metadata": {
        "id": "9RBIjf5TAafU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create csv for submission to Kaggle contest (for test data)\n",
        "\n",
        "import csv\n",
        "\n",
        "with open('output.csv', mode='w', newline='') as file:\n",
        "  # Create a CSV writer object\n",
        "  csv_writer = csv.writer(file)\n",
        "\n",
        "  # Write the rows to the CSV file\n",
        "  csv_writer.writerows(ansBool)"
      ],
      "metadata": {
        "id": "H1P5hyTOBJne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for validation, compare model output to is_correct field\n",
        "# note: does not work for test data as is_correct is TRUE for those data\n",
        "\n",
        "correct = 0\n",
        "\n",
        "for i in range(nData):\n",
        "\n",
        "  pred_answer = ans[i][1]\n",
        "  true_answer = str(val_dataset[i]['is_correct'])\n",
        "  if pred_answer == true_answer:\n",
        "    correct += 1\n",
        "\n",
        "print(correct)\n"
      ],
      "metadata": {
        "id": "2NzLhdpXBVA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save model to HuggingFace"
      ],
      "metadata": {
        "id": "d47fsFGMByO_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# login to huggingface to get write access\n",
        "\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()\n"
      ],
      "metadata": {
        "id": "PVDnewhCB3iF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save model and tokenizer to huggingface data base\n",
        "\n",
        "model.push_to_hub(\"suzoliver/Nov14_r64_2\")\n",
        "tokenizer.push_to_hub(\"suzoliver/Nov14_r64_2\")"
      ],
      "metadata": {
        "id": "JrNSefL2B_Bs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}